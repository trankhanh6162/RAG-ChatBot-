from langchain_community.llms import CTransformers
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import os

# --- Cáº¥u hÃ¬nh ---
model_file = "models/vinallama-7b-chat_q5_0.gguf"
vector_db_path = "vectorstores/db_faiss"

# --- Kiá»ƒm tra model ---
if not os.path.exists(model_file):
    raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y model táº¡i {model_file}")

# --- Load LLM ---
def load_llm(model_file):
    llm = CTransformers(
        model=model_file,
        model_type="llama",
        max_new_tokens=512,
        temperature=0
    )
    return llm

# --- Táº¡o Prompt Template ---
def create_prompt():
    template = """<|im_start|>system
Báº¡n lÃ  má»™t trá»£ lÃ½ AI. Chá»‰ sá»­ dá»¥ng thÃ´ng tin sau Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. 
Náº¿u báº¡n khÃ´ng biáº¿t cÃ¢u tráº£ lá»i, hÃ£y tráº£ lá»i "TÃ´i khÃ´ng biáº¿t". 
KhÃ´ng Ä‘Æ°á»£c suy Ä‘oÃ¡n hoáº·c táº¡o ra cÃ¢u tráº£ lá»i tá»« trÃ­ tÆ°á»Ÿng tÆ°á»£ng.

{context}<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant"""
    return PromptTemplate(template=template, input_variables=["context", "question"])

# --- Táº¡o QA Chain ---
def create_qa_chain(prompt, llm, db):
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=False,
        chain_type_kwargs={"prompt": prompt}
    )
    return qa_chain

# --- Load VectorDB ---
def read_vectors_db():
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    if not os.path.exists(vector_db_path):
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y vector DB táº¡i {vector_db_path}")
    db = FAISS.load_local(vector_db_path, embedding_model, allow_dangerous_deserialization=True)
    return db

# --- Khá»Ÿi cháº¡y ---
try:
    print("ğŸ”„ Äang táº£i VectorDB...")
    db = read_vectors_db()
    print("âœ… VectorDB Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng.")

    print("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh...")
    llm = load_llm(model_file)
    print("âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng.")

    print("ğŸ”„ Äang táº¡o prompt...")
    prompt = create_prompt()

    print("ğŸ”„ Äang táº¡o QA chain...")
    qa_chain = create_qa_chain(prompt, llm, db)

    # --- Äáº·t cÃ¢u há»i ---
    question = "Sá»± kiá»‡n lá»‹ch sá»­ nÄƒm 1908 á»Ÿ Viá»‡t Nam lÃ  gÃ¬?"
    response = qa_chain.invoke({"query": question})  # âœ… Sá»­a lá»—i táº¡i Ä‘Ã¢y

    print(f"âœ… CÃ¢u tráº£ lá»i: {response}")

except FileNotFoundError as e:
    print(f"âŒ Lá»—i: {e}")
except Exception as e:
    print(f"âŒ ÄÃ£ xáº£y ra lá»—i: {e}")
